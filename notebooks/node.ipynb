{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "epsilon_node_last.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNv4j8jjKCu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from scipy import interp\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjRPCUCLO6fF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6bb0184b-e551-4bf5-9ce1-348c38d6d8db"
      },
      "source": [
        "pip install scikit-optimize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/87/310b52debfbc0cb79764e5770fa3f5c18f6f0754809ea9e2fc185e1b67d3/scikit_optimize-0.7.4-py2.py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 25.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.16.0)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-20.4.0 scikit-optimize-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU293xFYN_Uw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "42412d5c-ebe7-4404-9e61-9b377d517062"
      },
      "source": [
        "!git clone https://github.com/Qwicen/node.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'node'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 33 (delta 8), reused 16 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnfbKsIWLmn-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2855bfd4-aace-4617-9bd3-ffc0fe3bd9dd"
      },
      "source": [
        "!pip install -r node/requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/facebookresearch/qhoptim/archive/master.zip (from -r node/requirements.txt (line 14))\n",
            "\u001b[?25l  Downloading https://github.com/facebookresearch/qhoptim/archive/master.zip\n",
            "\u001b[K     - 8.7MB 3.6MB/s\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 1)) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy>=0.13 in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 2)) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 4)) (0.22.2.post1)\n",
            "Collecting catboost==0.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/03/777a0e1c12571a7f3320a4fa6d5f123dba2dd7c0bca34f4f698a6396eb48/catboost-0.12.2-cp36-none-manylinux1_x86_64.whl (55.5MB)\n",
            "\u001b[K     |████████████████████████████████| 55.5MB 51kB/s \n",
            "\u001b[?25hCollecting xgboost==0.81\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/21/8b2ec99862903a6d3aed62ce156d21d114b8666e669c46d9e54041df9496/xgboost-0.81-py2.py3-none-manylinux1_x86_64.whl (16.6MB)\n",
            "\u001b[K     |████████████████████████████████| 16.6MB 206kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 8)) (4.41.1)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 10)) (1.0.5)\n",
            "Requirement already satisfied: prefetch_generator in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 11)) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r node/requirements.txt (line 12)) (2.23.0)\n",
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/57/fcef41c248701ee62e8325026b90c432adea35555cbc870aff9cfba23727/category_encoders-2.2.2-py2.py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->-r node/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17->-r node/requirements.txt (line 4)) (0.16.0)\n",
            "Collecting enum34\n",
            "  Downloading https://files.pythonhosted.org/packages/63/f6/ccb1c83687756aeabbf3ca0f213508fcfb03883ff200d201b3a4c60cedcc/enum34-1.1.10-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost==0.12.2->-r node/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r node/requirements.txt (line 7)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r node/requirements.txt (line 7)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r node/requirements.txt (line 7)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r node/requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r node/requirements.txt (line 9)) (3.12.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r node/requirements.txt (line 10)) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r node/requirements.txt (line 12)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r node/requirements.txt (line 12)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r node/requirements.txt (line 12)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r node/requirements.txt (line 12)) (2.10)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders->-r node/requirements.txt (line 13)) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders->-r node/requirements.txt (line 13)) (0.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->-r node/requirements.txt (line 9)) (49.6.0)\n",
            "Building wheels for collected packages: qhoptim\n",
            "  Building wheel for qhoptim (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qhoptim: filename=qhoptim-1.1.0-cp36-none-any.whl size=20328 sha256=a80f9671f0bb2338eb4dff10c8958c85fa573070cbd91f63799d3ba2178f657e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8dkisblp/wheels/63/53/2b/3b721f6feacd39063f6300e890b7cf633910a33e7b996edbd3\n",
            "Successfully built qhoptim\n",
            "Installing collected packages: enum34, catboost, xgboost, tensorboardX, category-encoders, qhoptim\n",
            "  Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed catboost-0.12.2 category-encoders-2.2.2 enum34-1.1.10 qhoptim-1.1.0 tensorboardX-2.1 xgboost-0.81\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYVVUHMYO2rH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "64355071-71ca-443b-eda5-5a28a5fa3650"
      },
      "source": [
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%env CUDA_VISIBLE_DEVICES=0,1\n",
        "import os, sys\n",
        "sys.path.insert(0, '..')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from node import lib\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from qhoptim.pyt import QHAdam\n",
        "\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from skopt.space import Integer\n",
        "from skopt.space import Real\n",
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize\n",
        "import time\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0,1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4UJD07_osxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SNin58sR7lZ",
        "colab_type": "text"
      },
      "source": [
        "## Create a predict_proba function to extract probabilities from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkdAiCZNZAOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_proba(self, X_test, y_test, device, batch_size=32):\n",
        "    X_test = torch.as_tensor(X_test, device=device)\n",
        "    y_test = lib.utils.check_numpy(y_test)\n",
        "    self.model.train(False)\n",
        "    with torch.no_grad():\n",
        "        logits = F.softmax(lib.utils.process_in_chunks(self.model, X_test, batch_size=batch_size), dim=1)\n",
        "        logits = lib.utils.check_numpy(logits)\n",
        "    return logits"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARG7FjFvZVIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lib.Trainer.predict_proba = predict_proba"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prd0w8V_0zIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files_path = '/path/to/datasets'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg5lKOKlrS8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files_list = os.listdir(files_path)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERaCcPW4rTFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_dataset(folder_path, filename):\n",
        "    df = pd.read_csv(os.path.join(folder_path, filename))\n",
        "    return df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJzGubZfrTIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dummy_encode(df):\n",
        "    cols_to_encode = list(df.select_dtypes(include=['category','object']))\n",
        "    if len(cols_to_encode): \n",
        "        df = pd.get_dummies(df, columns = cols_to_encode, prefix=cols_to_encode)\n",
        "\n",
        "    return df.values\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8LwczKWrWNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(df, target_col=None):\n",
        "    df = df.fillna(df.mean())\n",
        "    if target_col:\n",
        "        X = df.iloc[:, df.columns != target_col]\n",
        "        y = df.loc[:, df.columns == target_col]\n",
        "    else:\n",
        "        X = df.iloc[:, :-1]\n",
        "        y = df.iloc[:, -1]\n",
        "    lb = LabelEncoder()\n",
        "    y = lb.fit_transform(y)\n",
        "    X = dummy_encode(X)\n",
        "    classes = np.unique(y)\n",
        "\n",
        "    return X, y, classes\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orhq-X7Wo23p",
        "colab_type": "text"
      },
      "source": [
        "# Metrics calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8HExOYurWLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_calc(y_true, y_pred, classes):\n",
        "\n",
        "    y_true_label = label_binarize(y_true, classes)\n",
        "\n",
        "    y_pred_arg = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    cnf_matrix = confusion_matrix(y_true, y_pred_arg)\n",
        "\n",
        "    fp = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
        "    fn = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "    tp = np.diag(cnf_matrix)\n",
        "    tn = cnf_matrix.sum() - (fp + fn + tp)\n",
        "\n",
        "    fp = fp.astype(float)\n",
        "    fn = fn.astype(float)\n",
        "    tp = tp.astype(float)\n",
        "    tn = tn.astype(float)\n",
        "\n",
        "    tpr = tp/(tp+fn)\n",
        "    fpr = fp/(fp+tn)\n",
        "    precision = tp/(tp+fp)\n",
        "    acc = (tp+tn)/(tp+fp+fn+tn)\n",
        "\n",
        "    mean_acc = np.nanmean(acc)\n",
        "    mean_tpr = np.nanmean(tpr)\n",
        "    mean_fpr = np.nanmean(fpr)\n",
        "    mean_precision = np.nanmean(precision)\n",
        "\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc_list = []\n",
        "    pr_auc_list = []\n",
        "\n",
        "    if len(classes) == 2:\n",
        "        fpr, tpr, threshold_roc = roc_curve(y_true, y_pred[:, 1])\n",
        "\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        roc_auc_list.append(roc_auc)\n",
        "        \n",
        "        precision, recall, threshold_pr = precision_recall_curve(y_true,\n",
        "                                                            y_pred[:, 1])\n",
        "        \n",
        "        pr_auc = auc(recall, precision)\n",
        "        pr_auc_list.append(pr_auc)\n",
        "\n",
        "    else:\n",
        "        for i in range(y_pred.shape[1]):\n",
        "            fpr[i], tpr[i], threshold_roc = roc_curve(y_true_label[:, i], y_pred[:, i])\n",
        "            # roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "            roc_auc = auc(fpr[i], tpr[i])\n",
        "            roc_auc_list.append(roc_auc)\n",
        "            \n",
        "            precision[i], recall[i], threshold_pr = precision_recall_curve(y_true_label[:, i],\n",
        "                                                                y_pred[:, i])\n",
        "\n",
        "            pr_auc = auc(recall[i], precision[i])\n",
        "            pr_auc_list.append(pr_auc)\n",
        "\n",
        "    mean_roc_auc = np.nanmean(roc_auc_list)\n",
        "    mean_pr_auc = np.nanmean(pr_auc_list)\n",
        "\n",
        "    print('mean roc auc',mean_roc_auc)\n",
        "    print('mean pr auc', mean_pr_auc)\n",
        "\n",
        "    return mean_acc, mean_tpr, mean_fpr, mean_precision, mean_roc_auc, mean_pr_auc\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9NUoR3qSVp5",
        "colab_type": "text"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZUPwVGGEXAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(data, experiment_name, depth, num_layers):\n",
        "    num_features = data.X_train.shape[1]\n",
        "    num_classes = len(set(data.y_train) | set(data.y_test))\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        lib.DenseBlock(num_features, layer_dim=128, num_layers=num_layers,\n",
        "                    tree_dim=num_classes + 1, flatten_output=False,\n",
        "                    depth=depth, choice_function=lib.entmax15, bin_function=lib.entmoid15),\n",
        "        lib.Lambda(lambda x: x[..., :num_classes].mean(dim=-2)),\n",
        "    ).to(device)\n",
        "\n",
        "    model = model.double()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        res = model(torch.as_tensor(data.X_train[:2000], device=device))\n",
        "        \n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "\n",
        "    trainer = lib.Trainer(\n",
        "        model=model, loss_function=F.cross_entropy,\n",
        "        experiment_name=experiment_name,\n",
        "        warm_start=False,\n",
        "        Optimizer=QHAdam,\n",
        "        optimizer_params=dict(nus=(0.7, 1.0), betas=(0.95, 0.998)),\n",
        "        verbose=True,\n",
        "        n_last_checkpoints=5\n",
        "    )\n",
        "\n",
        "    return trainer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1hA6JFBoyU-",
        "colab_type": "text"
      },
      "source": [
        "#Hyperparameters optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq4mpUM8DZUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hyper_optimization(file, X, y):\n",
        "    search_space = [Integer(2, 8 , name='depth', dtype=int),\n",
        "                    Integer(2, 4, name='num_layers', dtype=int)]\n",
        "\n",
        "    @use_named_args(search_space)\n",
        "    def evaluate_model(**params):\n",
        "        results = []\n",
        "        kf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
        "        kf.get_n_splits(X)\n",
        "        for train_index, test_index in kf.split(X, y):\n",
        "\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            X_, X_val, y_, y_val = train_test_split(X_train, y_train, \n",
        "                                                              test_size=0.1, shuffle=True, \n",
        "                                                              random_state=42)\n",
        "\n",
        "\n",
        "            data = lib.Dataset(file, data_path=files_path,  X_train=X_train, y_train=y_train, \n",
        "                   X_valid=X_val, y_valid=y_val, X_test=X_test, y_test=y_test,\n",
        "                   random_state=1337, quantile_transform=True)\n",
        "\n",
        "            experiment_name = 'epsilon_node.{}'.format(time.time())\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "            model = create_model(data, experiment_name, **params)\n",
        "\n",
        "            for batch in lib.iterate_minibatches(data.X_train, data.y_train, batch_size=32, \n",
        "                                                shuffle=True, epochs=float(5)):\n",
        "                \n",
        "                metrics = model.train_on_batch(*batch, device=device)\n",
        "            \n",
        "            y_pred = model.predict_proba(data.X_test, data.y_test,\n",
        "                                         device=device, batch_size=32)\n",
        "            y_pred_arg = np.argmax(y_pred, axis=1)\n",
        "\n",
        "            result = accuracy_score(y_test, y_pred_arg)\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "        # calculate the mean of the scores\n",
        "        estimate = mean(results)\n",
        "        print('mean resutls', estimate)\n",
        "        return 1.0 - estimate\n",
        "\n",
        "    # perform optimization\n",
        "    result = gp_minimize(evaluate_model, search_space, n_calls=50)\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh-9uSclScU5",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6lspceKrLb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for file in files_list:\n",
        "    print('file', file)\n",
        "\n",
        "    df = read_dataset(files_path, file)\n",
        "\n",
        "    if file == 'analcatdata_germangss.csv':\n",
        "        X, y, classes = prepare_data(df, 'Political_system')\n",
        "    else:\n",
        "        X, y, classes = prepare_data(df)\n",
        "\n",
        "    result = hyper_optimization(file, X, y)\n",
        "\n",
        "    best_params_vals = result.x\n",
        "    params_name = ['depth', 'num_layers']\n",
        "    best_params = dict(zip(params_name, best_params_vals))\n",
        "\n",
        "    print(best_params)\n",
        "\n",
        "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "    kf.get_n_splits(X)\n",
        "\n",
        "    acc_list = []\n",
        "    tpr_list = []\n",
        "    fpr_list = []\n",
        "    precision_list = []\n",
        "    roc_auc_list = []\n",
        "    pr_auc_list = []\n",
        "    training_time_list = []\n",
        "    inference_time_list = []\n",
        "    cross_val = []\n",
        "\n",
        "    for fold_num, data_index in enumerate(kf.split(X, y)):\n",
        "        train_index, test_index = data_index[0], data_index[1]\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        X_, X_val, y_, y_val = train_test_split(X_train, y_train,\n",
        "                                                          test_size=0.1,\n",
        "                                                          shuffle=True,\n",
        "                                                          random_state=42)\n",
        "\n",
        "\n",
        "        data = lib.Dataset(file, data_path=files_path,  X_train=X_train, y_train=y_train, \n",
        "                X_valid=X_val, y_valid=y_val, X_test=X_test, y_test=y_test,\n",
        "                random_state=1337, quantile_transform=True)\n",
        "        experiment_name = 'epsilon_node.{}'.format(time.time())\n",
        "\n",
        "        model = create_model(data, experiment_name, **best_params)\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        for batch in lib.iterate_minibatches(data.X_train, data.y_train,\n",
        "                                             batch_size=32, shuffle=True,\n",
        "                                             epochs=float(20)\n",
        "                                             ):\n",
        "            metrics = model.train_on_batch(*batch, device=device)\n",
        "\n",
        "        t1 = time.time()\n",
        "        train_time = t1 - t0\n",
        "        print('train time', train_time)\n",
        "\n",
        "        t2 = time.time()\n",
        "        y_pred = model.predict_proba(data.X_test, data.y_test,\n",
        "                                        device=device, batch_size=128)\n",
        "        t3 = time.time()\n",
        "        inference_time = (t3 - t2) * 1000 / y_test.shape[0]\n",
        "\n",
        "        acc, mean_tpr, mean_fpr, mean_precision, mean_roc_auc, mean_pr_auc = metrics_calc(y_test, y_pred, classes)\n",
        "\n",
        "        acc_list.append(\"{:.3f}\".format(acc))\n",
        "        tpr_list.append(\"{:.3f}\".format(mean_tpr))\n",
        "        fpr_list.append(\"{:.3f}\".format(mean_fpr))\n",
        "        precision_list.append(\"{:.3f}\".format(mean_precision))\n",
        "        roc_auc_list.append(\"{:.3f}\".format(mean_roc_auc))\n",
        "        pr_auc_list.append(\"{:.3f}\".format(mean_pr_auc))\n",
        "        training_time_list.append(\"{:.1f}\".format(train_time))\n",
        "        inference_time_list.append(\"{:.1f}\".format(inference_time))\n",
        "\n",
        "        cross_val.append(fold_num + 1)\n",
        "\n",
        "    results_dict = {'Dataset Name':[file.split('.')[0]] * 10,\n",
        "                'Algorithm Name':['node'] * 10,\n",
        "                'Cross Validation':cross_val,\n",
        "                'Hyper Parameters Values': [best_params_vals] * 10,\n",
        "                'Accuracy':acc_list,\n",
        "                'tpr':tpr_list,\n",
        "                'FPR':fpr_list,\n",
        "                'Precision':precision_list,\n",
        "                'AUC':roc_auc_list,\n",
        "                'PR-Curve':pr_auc_list,\n",
        "                'Training Time':training_time_list,\n",
        "                'Inference Time':inference_time_list\n",
        "                }\n",
        "\n",
        "    df_res = pd.DataFrame.from_dict(results_dict)\n",
        "    df_res = df_res.fillna(0)\n",
        "\n",
        "    df_res.to_csv('path/to/results',\n",
        "              mode='a', header=False)\n",
        "    \n",
        "    model.zero_grad()\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}